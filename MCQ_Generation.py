# -*- coding: utf-8 -*-
"""
Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1V2-zuR7LclhpqRQrAAhg5kAGCxJhZkH1

## Installation and mount Google Drive
"""

# !python MCQ_T5_spaCy_BERT_BART.py model_dir mode_of_input mcqs_path chapter_path text_file_path
import argparse
parser = argparse.ArgumentParser()
parser.add_argument("model_dir", type=str,
                    help="Directory of Bert Word Sense Disambiguation Model")
parser.add_argument("mode_of_input",type=str,
                    help=" 'chapter' if generate from chapter, 'inshorts' if generate from inshorts article")

parser.add_argument("mcqs_path",type=str,
                    help="Directory where generated mcqs must be kept in text format")
parser.add_argument("chapter_path",type=str,
                    help="Path of the pdf if chapter input mode is a chapter else None")
parser.add_argument("text_file_path",type=str,
                    help="Directory where text file generated after parsing the pdf must be kept")
args = parser.parse_args()

#!pip install --quiet transformers==2.9.0
#!pip install --quiet nltk==3.4.5
#!pip install word2number

#!pip install PyMuPDF

from operator import itemgetter
import fitz

#!pip install spacy==2.1.0
#!pip install neuralcoref
#!python -m spacy download en
"""## **DISTRACTORS ARE OTHER OPTIONS SIMILAR TO CORRECT OPTION (INCORRECT OPTIONS)**"""

# Distractors from Wordnet
def get_distractors_wordnet(syn,word):
    distractors=[]
    word= word.lower()
    orig_word = word
    if len(word.split())>0:
        word = word.replace(" ","_")
    hypernym = syn.hypernyms()
    if len(hypernym) == 0: 
        return distractors
    for item in hypernym[0].hyponyms():
        name = item.lemmas()[0].name()
        if name == orig_word:
            continue
        name = name.replace("_"," ")
        name = " ".join(w.capitalize() for w in name.split())
        if name is not None and name not in distractors:
            distractors.append(name)
    return distractors

import os

"""## Find the correct sense (contextual meaning) of a given word in a sentence"""

import torch
import math
from transformers import BertModel, BertConfig, BertPreTrainedModel, BertTokenizer

class BertWSD(BertPreTrainedModel):
    def __init__(self, config):
        super().__init__(config)

        self.bert = BertModel(config)
        self.dropout = torch.nn.Dropout(config.hidden_dropout_prob)

        self.ranking_linear = torch.nn.Linear(config.hidden_size, 1)

        self.init_weights()

DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model_dir =args.model_dir   # "/content/gdrive/MyDrive/Exam_lounge/MCQ Generation/bert_base-augmented-batch_size=128-lr=2e-5-max_gloss=6"


model = BertWSD.from_pretrained(model_dir)
tokenizer = BertTokenizer.from_pretrained(model_dir)
# add new special token
if '[TGT]' not in tokenizer.additional_special_tokens:
    tokenizer.add_special_tokens({'additional_special_tokens': ['[TGT]']})
    assert '[TGT]' in tokenizer.additional_special_tokens
    model.resize_token_embeddings(len(tokenizer))
    
model.to(DEVICE)
model.eval()

import csv
import os
from collections import namedtuple

import nltk
nltk.download('wordnet')
from nltk.corpus import wordnet as wn

import torch
from tqdm import tqdm

GlossSelectionRecord = namedtuple("GlossSelectionRecord", ["guid", "sentence", "sense_keys", "glosses", "targets"])
BertInput = namedtuple("BertInput", ["input_ids", "input_mask", "segment_ids", "label_id"])



def _create_features_from_records(records, max_seq_length, tokenizer, cls_token_at_end=False, pad_on_left=False,
                                  cls_token='[CLS]', sep_token='[SEP]', pad_token=0,
                                  sequence_a_segment_id=0, sequence_b_segment_id=1,
                                  cls_token_segment_id=1, pad_token_segment_id=0,
                                  mask_padding_with_zero=True, disable_progress_bar=False):

    features = []
    for record in tqdm(records, disable=disable_progress_bar):
        tokens_a = tokenizer.tokenize(record.sentence)

        sequences = [(gloss, 1 if i in record.targets else 0) for i, gloss in enumerate(record.glosses)]

        pairs = []
        for seq, label in sequences:
            tokens_b = tokenizer.tokenize(seq)

            _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)

            tokens = tokens_a + [sep_token]
            segment_ids = [sequence_a_segment_id] * len(tokens)

            tokens += tokens_b + [sep_token]
            segment_ids += [sequence_b_segment_id] * (len(tokens_b) + 1)

            if cls_token_at_end:
                tokens = tokens + [cls_token]
                segment_ids = segment_ids + [cls_token_segment_id]
            else:
                tokens = [cls_token] + tokens
                segment_ids = [cls_token_segment_id] + segment_ids

            input_ids = tokenizer.convert_tokens_to_ids(tokens)

            # The mask has 1 for real tokens and 0 for padding tokens. Only real
            # tokens are attended to.
            input_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)

            # Zero-pad up to the sequence length.
            padding_length = max_seq_length - len(input_ids)
            if pad_on_left:
                input_ids = ([pad_token] * padding_length) + input_ids
                input_mask = ([0 if mask_padding_with_zero else 1] * padding_length) + input_mask
                segment_ids = ([pad_token_segment_id] * padding_length) + segment_ids
            else:
                input_ids = input_ids + ([pad_token] * padding_length)
                input_mask = input_mask + ([0 if mask_padding_with_zero else 1] * padding_length)
                segment_ids = segment_ids + ([pad_token_segment_id] * padding_length)

            assert len(input_ids) == max_seq_length
            assert len(input_mask) == max_seq_length
            assert len(segment_ids) == max_seq_length

            pairs.append(
                BertInput(input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids, label_id=label)
            )

        features.append(pairs)

    return features


def _truncate_seq_pair(tokens_a, tokens_b, max_length):
    """Truncates a sequence pair in place to the maximum length."""
    while True:
        total_length = len(tokens_a) + len(tokens_b)
        if total_length <= max_length:
            break
        if len(tokens_a) > len(tokens_b):
            tokens_a.pop()
        else:
            tokens_b.pop()

import re
import torch
from tabulate import tabulate
from torch.nn.functional import softmax
from tqdm import tqdm
from transformers import BertTokenizer
import time


MAX_SEQ_LENGTH = 128

def get_sense(sent):
  re_result = re.search(r"\[TGT\](.*)\[TGT\]", sent)
  if re_result is None:
      print("\nIncorrect input format. Please try again.")

  ambiguous_word = re_result.group(1).strip()

  results = dict()

  wn_pos = wn.NOUN
  for i, synset in enumerate(set(wn.synsets(ambiguous_word, pos=wn_pos))):
      results[synset] =  synset.definition()

  if len(results) ==0:
    return (None,None,ambiguous_word)

  # print (results)
  sense_keys=[]
  definitions=[]
  for sense_key, definition in results.items():
      sense_keys.append(sense_key)
      definitions.append(definition)


  record = GlossSelectionRecord("test", sent, sense_keys, definitions, [-1])

  features = _create_features_from_records([record], MAX_SEQ_LENGTH, tokenizer,
                                            cls_token=tokenizer.cls_token,
                                            sep_token=tokenizer.sep_token,
                                            cls_token_segment_id=1,
                                            pad_token_segment_id=0,
                                            disable_progress_bar=True)[0]

  with torch.no_grad():
      logits = torch.zeros(len(definitions), dtype=torch.double).to(DEVICE)
      # for i, bert_input in tqdm(list(enumerate(features)), desc="Progress"):
      for i, bert_input in list(enumerate(features)):
          logits[i] = model.ranking_linear(
              model.bert(
                  input_ids=torch.tensor(bert_input.input_ids, dtype=torch.long).unsqueeze(0).to(DEVICE),
                  attention_mask=torch.tensor(bert_input.input_mask, dtype=torch.long).unsqueeze(0).to(DEVICE),
                  token_type_ids=torch.tensor(bert_input.segment_ids, dtype=torch.long).unsqueeze(0).to(DEVICE)
              )[1]
          )
      scores = softmax(logits, dim=0)

      preds = (sorted(zip(sense_keys, definitions, scores), key=lambda x: x[-1], reverse=True))


  # print (preds)
  sense = preds[0][0]
  meaning = preds[0][1]
  return (sense,meaning,ambiguous_word)


from transformers import T5ForConditionalGeneration,T5Tokenizer

question_model = T5ForConditionalGeneration.from_pretrained('ramsrigouthamg/t5_squad_v1')
question_tokenizer = T5Tokenizer.from_pretrained('t5-base')

def get_question(sentence,answer):
  text = "context: {} answer: {} </s>".format(sentence,answer)
  print (text)
  max_len = 256
  encoding = question_tokenizer.encode_plus(text,max_length=max_len, pad_to_max_length=True, return_tensors="pt")

  input_ids, attention_mask = encoding["input_ids"], encoding["attention_mask"]

  outs = question_model.generate(input_ids=input_ids,
                                  attention_mask=attention_mask,
                                  early_stopping=True,
                                  num_beams=5,
                                  num_return_sequences=1,
                                  no_repeat_ngram_size=2,
                                  max_length=200)


  dec = [question_tokenizer.decode(ids) for ids in outs]


  Question = dec[0].replace("question:","")
  Question= Question.strip()
  return Question

"""## Putting it all together"""

def getMCQs(sent):
  sentence_for_bert = sent.replace("**"," [TGT] ")
  sentence_for_bert = " ".join(sentence_for_bert.split())
  # try:
  sense,meaning,answer = get_sense(sentence_for_bert)
  if sense is not None:
    distractors = get_distractors_wordnet(sense,answer)
  else: 
    distractors = ["Word not found in Wordnet. So unable to extract distractors."]
  sentence_for_T5 = sent.replace("**"," ")
  sentence_for_T5 = " ".join(sentence_for_T5.split()) 
  ques = get_question(sentence_for_T5,answer)
  return ques,answer,distractors,meaning



def fonts(doc, granularity=False):
    styles = {}
    font_counts = {}

    for page in doc:
        blocks = page.getText("dict")["blocks"]
        for b in blocks:  # iterate through the text blocks
            if b['type'] == 0:  # block contains text
                for l in b["lines"]:  # iterate through the text lines
                    for s in l["spans"]:  # iterate through the text spans
                        if granularity:
                            identifier = "{0}_{1}_{2}_{3}".format(s['size'], s['flags'], s['font'], s['color'])
                            styles[identifier] = {'size': s['size'], 'flags': s['flags'], 'font': s['font'],
                                                  'color': s['color']}
                        else:
                            identifier = "{0}".format(s['size'])
                            styles[identifier] = {'size': s['size'], 'font': s['font']}

                        font_counts[identifier] = font_counts.get(identifier, 0) + 1  # count the fonts usage

    font_counts = sorted(font_counts.items(), key=itemgetter(1), reverse=True)

    if len(font_counts) < 1:
        raise ValueError("Zero discriminating fonts found!")

    return font_counts, styles

def create_doc_from_chapter(jess1_path,text_file_path):
  doc = fitz.open(jess1_path)
  font_counts, styles = fonts(doc, granularity=False)
  new_doc = ''
  for page in doc:
      blocks = page.getText("dict")["blocks"]
      for b in blocks:  # iterate through the text blocks
          if b['type'] == 0:  # block contains text
              for l in b["lines"]:  # iterate through the text lines
                  for s in l["spans"]:
                    if s['size']==11.5:
                      new_doc = new_doc +' '+ s['text']
  new_doc
  text_file = open(text_file_path, "w")
  text_file.write(new_doc)
  text_file.close()
  
  f = open(text_file_path,'r')
  doc1 = f.readlines()[0]
  doc1 = re.sub(r"\([^)]*\)", "",doc1)
  return doc1
import re
import requests
from bs4 import BeautifulSoup

def create_topic_dict_from_inshorts():
  content_list = ['sports','national','business','world','politics','technology','startup','entertainment','miscellaneous','hatke','science','automobile']
  HEADERS = ({'User-Agent':'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36','Accept-Language': 'en-US, en;q=0.5'})
  topic_dict = dict()
  for content in content_list:

    temp = list()

    scrap_link = "http://www.inshorts.com/en/read/"+content
    page = requests.get(scrap_link, headers=HEADERS)
    soup = BeautifulSoup(page.content, features="lxml")

    for i in soup.findAll("div", attrs={"itemprop":"articleBody"}):
      temp.append(i.text)

    topic_dict[content] = temp
  return topic_dict  

import neuralcoref
import spacy
nlp = spacy.load('en')
neuralcoref.add_to_pipe(nlp)

from datetime import date

if not os.path.isdir(args.mcqs_path):
  os.mkdir(args.mcqs_path)
import codecs

def read_path(path):
  with codecs.open(path, 'r', encoding='utf-8',errors='ignore') as fdata:
    article = fdata.read()
  return article

def cum_doc_and_mcq_files(mcqs_path,base_path=None,key = 'sports',list_of_articles=None,read_from_base=False,summarizer = False):
  cum_doc = []
  mcq_files = []
  mcqs_path = mcqs_path+str(date.today().day)+" "+date.today().strftime("%B")+"/"
  g=1
  if read_from_base:
    for i in range(5):
      path = base_path+"article"+str(i)+".txt"
      article = read_path(path)
      article = article.replace('\n',' ').replace('\r','')
      if summarizer:
        try:
          article = summarizer(article)
        except:
          pass
      doc = nlp(article)
      cum_doc.append(doc)
    mcq_file = mcqs_path+"mcq_article"+str(i)+".txt"
    mcq_files.append(mcq_file)
  else:
    for i in list_of_articles:
      article = i.replace("...","").replace("last year","2020")
      if summarizer:
        try:
          article = summarizer(article)
        except:
          pass
      doc = nlp(article)
      cum_doc.append(doc)
      if not os.path.isdir(mcqs_path+key):
        os.mkdir(mcqs_path+key)
      mcq_file = mcqs_path+key+"/"+"article"+str(g)+".txt"
      g=g+1
      mcq_files.append(mcq_file)

  return cum_doc,mcq_files

import numpy as np

"""# Core section"""

import re

def format_special_characters(text): 
    pat = '[ ]'+r'[^a-zA-z0-9<>]'+'[ ]*'
    return re.findall(pat,text)

from collections import Counter



from word2number import w2n

import random
def inputs_and_dictionary(doc,dictionary=None):
  inputs = []
  if dictionary == None:
    dictionary = {}
    dictionary["Person"]=[]
    dictionary["Date"]=[]
    dictionary["Place"]=[]
    dictionary["Org"]=[]
  items = [x.text for x in doc.ents]
  for sent in doc.sents:
    entities = sent.ents
    temp = []
    sent_input= []
    for token in sent:
      if (token.text.lower() in ["he", "she", "it", "him", "they","them","this","i","we"]) and len(token._.coref_clusters):
        temp.append("<>".join(list(token._.coref_clusters[0][0].text)))
      elif  ( token.text.lower() in ["his", "her", "its","their","my","our"]) and (len(token._.coref_clusters))>0 and (token._.coref_clusters[0][0].text.lower() not in ["he", "she", "it", "him", "they","them","this"]):
        temp.append("<>".join(list(token._.coref_clusters[0][0].text)) + "'s")
      else :
        if (token.text.lower()=="am"):
          temp.append("is")
        else:
          temp.append(token.text)
      

    temp = ' '.join(temp)
    to_replace = format_special_characters(temp)
    for char in to_replace:
      temp = temp.replace(char,char.strip())
    if len(entities):
      for x in entities:
        temp1 = temp
        temp1 = temp1.replace(x.text,'**'+x.text+'**')
        temp1 = temp1.replace('<>','')
        if ("**" not in temp1):
          temp2 = temp
          temp2 = temp2.replace(x.text.replace(", ",","),'**'+x.text+'**')
          temp1 = temp2.replace('<>','')

        if (x.label_ == 'PERSON'):
          id = 'Person'
        elif (x.label_ == 'DATE'):
          id = 'Date'
        elif (x.label_ == 'GPE'):
          id = 'Place'
        elif (x.label_ == 'ORG'):
          id = 'Org'
        else:
          id = None
        if id :
          dictionary[id].append(x.text)
        if ("**" in temp1):
          input= [temp1,id if id else x.label_,x.text,Counter(items)[x.text]]
          sent_input.append(input)
      random.shuffle(sent_input)
      flag={}
      for (a,b,c,d) in sent_input:
        if (b=="CARDINAL"):
          try:
            cemo = w2n.word_to_num(c)
          except:
            cemo = 0
        if (b != "CARDINAL") or (b == "CARDINAL" and cemo >3):
          if b not in flag.keys():
            flag[b]=1
          

          if (b=='Date'):
            if any(chr.isdigit() for chr in x.text):
              inputs.append([a,b,c])
          else:
            if d < 2 :
              if flag[b]==1:
                inputs.append([a,b,c])
                flag[b]=0

  return dictionary, inputs

from dateutil import parser as date_parser
def parse_date_(date):
  date_time = date_parser.parse(date)
  try :
    year = int(date)
    month = None
    day = None
  except :
    year = int(date_time.year)
    month = int(date_time.month)
    day = int(date_time.day)

  return year,month,day
def generate_date(parsed_date):
  year,month,day = parsed_date
  list_year = list(range(1,10))
  random.shuffle(list_year)
  if month:
    list_month = [i for i in range(1,13) if (i!=month) and (i!=day)]
    random.shuffle(list_month)
  if day:
    list_day = [i for i in range(1,28) if (i!=month) and (i!=day)]
    random.shuffle(list_month)
  if year not in range(2010,2021):
    date_time1 = date_parser.parse(str(list_day[0] if day else 1)+"/"+str(list_month[0] if month else 1)+"/"+str(year+list_year[0] if random.random()>0.5 else year-list_year[0]))
    date_time2 = date_parser.parse(str(list_day[1] if day else 1)+"/"+str(list_month[1] if month else 1)+"/"+str(year+list_year[1] if random.random()>0.5 else year-list_year[1]))
  else:
    date_time1 = date_parser.parse(str(list_day[0] if day else 1)+"/"+str(list_month[0] if month else 1)+"/"+str(year-2 if random.random()>0.5 else year-1))
    date_time2 = date_parser.parse(str(list_day[1] if day else 1)+"/"+str(list_month[1] if month else 1)+"/"+str(year+1 if random.random()>0.5 else year-3))
 

  first = [date_time1.strftime("%d"),date_time1.strftime("%B"),date_time1.strftime("%Y")]
  second = [date_time2.strftime("%d"),date_time2.strftime("%B"),date_time2.strftime("%Y")]
  return [first,second]

import random

def generate_mcqs(doc,inputs,mcq_file):
  j=1
  tracker = 1
  with open(mcq_file, 'w') as f:
    if doc:
      f.write(doc.text)
    f.write('\n')
    for i in inputs:
      sentence = i[0]
      exceed_cardinal=0
      print("\n")
      print(i[1])
      if (i[1]!= "Date") or ((i[1]== "Date") and (any(chr.isdigit() for chr in i[2]))):
        question,answer,distractors,meaning = getMCQs(sentence)
        new_distractors =[]
        if i[1] in dictionary.keys():
          new_distractors = dictionary[i[1]].copy()
          random.shuffle(new_distractors)
          new_distractors.remove(i[2])
          if (i[1] == "Date"):
            try:
              date_distractors = generate_date(parse_date_(i[2]))
              k = [j for j in i[2].replace(","," ").split(" ") if len(j)!=0]

              if (i[2].isdigit()):
                date_distractors=[i[2] for i in date_distractors]
              elif (len(k)==2):
                if (len(k[0])==2):
                  date_distractors = [' '.join(i[:2]) for i in date_distractors]
                else:
                  date_distractors = [' '.join(i[2:]) for i in date_distractors]
              elif (i[2].split(" ")[1].lower() in ['year','years'] ):
                date_distractors = [random.choice([i for i in list(range(int(i[2].split(" ")[0])-5,int(i[2].split(" ")[0])+5)) if i!=int(i[2].split(" ")[0])]) for j in range(2)]
                date_distractors = [" ".join([str(k),i[2].split(" ")[1]]) for k in date_distractors]
              else:
                date_distractors=[' '.join(i) for i in date_distractors]
            except:
              if (i[2][-1]=='s' and i[2][-2]=='0'):
                date_distractors = [i[2][:-3]+str(int(i[2][-3])+1)+i[2][-2:],i[2][:-3]+str(int(i[2][-3])-1)+i[2][-2:]]
              else:
                date_distractors=[]
            date_distractors.extend(new_distractors)
            new_distractors = date_distractors
        elif (i[1]=="CARDINAL"):
          try:
            number = w2n.word_to_num(i[2])
            if number<5:
              cardinal_distractors = [1,2,3,4]
              cardinal_distractors = [c for c in np.unique(cardinal_distractors) if c!=number]
            
            elif number <10:
              cardinal_distractors = [5,6,7,8,9,10,11,12,13,14,15]
              cardinal_distractors = [c for c in np.unique(cardinal_distractors) if c!=number]     
            elif number<600:
              cardinal_distractors = [number+i*(1 if random.random() <0.5 else -1) for i in range(8)]
              cardinal_distractors = [c for c in np.unique(cardinal_distractors) if c>0.8*number and c!=number]
            else:
              exceed_cardinal = 1
            random.shuffle(cardinal_distractors)
            cardinal_distractors=cardinal_distractors[:4]
          except:
            cardinal_distractors=[]
          cardinal_distractors.extend(new_distractors)
          new_distractors = cardinal_distractors

        if (len(new_distractors)>=4 or len(distractors)>=4) and (exceed_cardinal==0):
          f.write('Que   %d :%s\n' % (j, question))
          f.write('Options :')
          for k in range(4):  
            if len(new_distractors)>=4:
              f.write('(%d) %s ' % (k+1,new_distractors[k] if k<len(new_distractors) else '----'))
            elif 'Wordnet' not in distractors[0]:
              f.write('(%d) %s ' % (k+1,distractors[k] if k<len(distractors) else '----'))
          f.write('\n    Ans :%s\n' % answer)
          tracker=tracker+1
          j=j+1    
  f.close()

"""# Main section"""

if __name__ == "__main__":
  if args.mode_of_input == 'inshorts':
    topic_dict=create_topic_dict_from_insorts()
    for key in topic_dict:
      cum_doc,mcq_files = cum_doc_and_mcq_files(key=key,mcqs_path=args.mcqs_path,list_of_articles=topic_dict[key])
      z=0
      cum_inputs=[]
      for doc in cum_doc:
        if z ==0:
          dictionary,inputs = inputs_and_dictionary(doc)
        else :
          dictionary,inputs = inputs_and_dictionary(doc,dictionary)
          cum_inputs.append(inputs)
          z=z+1  
      for i in dictionary:
        dictionary[i] = list(np.unique(dictionary[i]))
      for (doc,inputs,mcq_file) in zip(cum_doc,cum_inputs,mcq_files):
        generate_mcqs(doc,inputs,mcq_file)
  elif args.mode_of_input == 'chapter':
    doc = create_doc_from_chapter(args.chapter_path,args.text_file_path)
    doc = nlp(doc)
    dictionary,inputs = inputs_and_dictionary(doc)
    for i in dictionary:
      dictionary[i] = list(np.unique(dictionary[i]))
    generate_mcqs(None,inputs,args.mcqs_path+"mcqs.txt")
